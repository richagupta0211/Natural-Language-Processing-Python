#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#You can comment out the Word Cloud section at the end of the code before running the code.

#_______IMPORT PACKAGES_________#
import spacy
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from string import punctuation
from nltk.corpus import stopwords
from nltk import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
from wordcloud import WordCloud, ImageColorGenerator
from PIL import Image
import matplotlib.pyplot as plt
import os
from textatistic import Textatistic


#_______IMPORTING THE DATASET (CSV FILES)_________#
#Set this path to the folder which contains the csv files
path = "/Users/rashmigupta/Desktop/NLP/Final Project/csv"
os.chdir(path)

#This function reads in the comments csv, extracts comments column and joins them into a single file.
def read_csv(file):
    #Read the csv file
    df_csv = pd.read_csv(file)
    #extract the comment column
    comments_df = df_csv[['Comment']]
    #Flatten the data frame
    comments = comments_df.values.flatten()
    #Join all the comments with a space in between. Becomes a list
    comments_list = " ". join(comments)
    return comments_list



#Reading each csv file one by one and storing the comments extracted in a variable
fox_comments = read_csv('FoxNews.csv')
cnn_comments = read_csv('CNN.csv')
abc_comments = read_csv('ABC.csv')
cbs_comments = read_csv('CBS.csv')
nbc_comments = read_csv('NBC.csv')
cspan_comments = read_csv('CSPAN.csv')

#Check the length of each of them
len(fox_comments) #907202
len(cnn_comments) #660724
len(abc_comments) #100204
len(cbs_comments) #558904
len(nbc_comments) #476359
len(cspan_comments) #1491606
#ABC has the least number of words because ABC had only 3 videos which were relevant to the analysis.
#Fox News, CBS and NBC had 5 videos, CSPAN had 7 and CNN had 10 videos.
#You can read about the reason behind these numbers in the project paper.

#Initialize a dictionary with keys as channel names and comments scraped as values
comments_dict = {'FoxNews':fox_comments,
                 'CNN':cnn_comments,
                 'ABC':abc_comments,
                 'CBS':cbs_comments,
                 'NBC':nbc_comments,
                 'CSPAN':cspan_comments}



#_______CLEANUP_________#
#put the comments of each channel into a new list comments
comments = list(comments_dict.values())
len(comments) #6
#Stopwords
stop_words = stopwords.words('english') + list(punctuation)
#removing some other commonly occuring words that seem useless to our analysis. This was done
#retrospectively after looking at the most commonly ocurring words that seemed meaningless as such.
#This list can be added or removed from as per your analysis needs.
remove_words = ['re', "n't",'think', 'com', 'got', 'say', 'going', 'go', 'know', 'get', 'would', 'said', 'one', 'like', 'see', 'even', 'want', 'https', 'nothing']
#A new comments list to store the cleaned comments.
clean_comments = []
#This loop will run 6 times, once for each channel
for x in comments:
    #This will return a tokenized copy of the text
    x = word_tokenize(x)
    #converting all the words in lower case
    x = [w.lower() for w in x]
    #removing stop words
    x = [w for w in x if w not in stop_words]
    #remove words in remove_words list
    x = [w for w in x if w not in remove_words]
    #removing digits from our comments
    x = [w for w in x if w.isdigit() == 0]
    clean_comments.append(x)



#_______TFIDF_________#
#making a list of the channel names collecting the names from comments_dict dictionary keys
channels = list(comments_dict.keys())
#clean comments now stored with each variable which is the channel name.
#FOXNEWS is now a variable that stores the cleaned comments from FoxNews channel.
FOXNEWS, CNN, ABC, CBS, NBC, CSPAN = [y for y in clean_comments]

#checking the length of the cleaned comments. 
#This length might vary depending on the words that you have removed for cleaning in the remove_words list.
len(FOXNEWS) 
len(CNN) 
len(ABC) 
len(CBS) 
len(NBC) 
len(CSPAN) 
# ABC has the least number of words

#defining a vectorizer with ngram range from 1 to 3
vectorizer = TfidfVectorizer(ngram_range=(1, 3))
#making a list of the channel variables which store the cleaned comments
channels_list = [FOXNEWS, CNN, ABC, CBS, NBC, CSPAN]
#Transform the comments and apply the vectorizer
vectors = vectorizer.fit_transform([str(w) for w in channels_list])
#extract the feature names from the vectorizer
feature_names = vectorizer.get_feature_names()
#return a matrix of the vectors
dense = vectors.todense()
#make a list from the matrix
denselist = dense.tolist()

#making a data frame of the frequencies of the words for each channel comments
df = pd.DataFrame(denselist, columns=feature_names, index = channels)
#Transpose the dataframe
df_t = df.T
#select the 20 most frequently occuring words for each channel.
[df_t.nlargest(20,col) for col in df_t.columns]



#_______Looking for some interesting terms now_________#
terms = ['democrats', 'trump', 'impeachment', 'biden', 'hunter']
df_t.loc[terms]
#Both Hunter and Biden appear more in CNN, CBS and CSPAN

terms = ['ukraine', 'yovanovitch', 'burisma', 'zelensky', 'guliani']
df_t.loc[terms]
#


#_______NER_________#
#'en_core_web_sm' is spaCy's english language model. In this line I am loading it and naming the variable 'nlp'.
nlp = spacy.load('en_core_web_sm')
number = len(comments_dict.keys())
#creating dictionaries to store different types of entities for analysis later
uniq_ppl = {}
uniq_GPE = {}
uniq_NORP = {}
uniq_ORG = {}

#Loop will run 6 times, once for each channel
#In this I am extracting the names that have entities PERSON, GPE, NORP, ORG for each channel
for i in range(number):
    channel_name = list(comments_dict.keys())[i]
    #nlp worked only with 1000000 values at max
    text = nlp(list(comments_dict.values())[i][0:1000000])
    text_list = [(X.text, X.label_) for X in text.ents]
    text_ents = [x.label_ for x in text.ents]
    #append all with channel name
    people = []
    GPE = []
    NORP = []
    ORG = []
    for x in text_list:
        if x[1] == 'PERSON':
            people.append(x[0])
        if x[1] == 'GPE':
            GPE.append(x[0])
        if x[1] == 'NORP':
            NORP.append(x[0])
        if x[1] == 'ORG':
            ORG.append(x[0])
    #extracting distinct values
    uniq_ppl[i] = set(people)
    uniq_GPE[i] = set(GPE)
    uniq_NORP[i] = set(NORP)
    uniq_ORG[i] = set(ORG)  


# Find strings that are similar to "Trump" in the uniq_ppl dictionary. The function will give us top 20 matches.
# scorer = fuzz.ratio will compare the entire strings with each other. And give us a list of strings in the order in which
# they match the most to least. I am using this ratio because I want to match "Trump" completely with another string.
similar_strings_trump = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_trump[i] = process.extract("Trump", list(uniq_ppl.values())[i], limit=20, scorer=fuzz.ratio)

#Some funny words used for democrats
similar_strings_democrats = {}
for i in range(len(uniq_NORP.keys())):
    similar_strings_democrats[i] = process.extract("Democrats", list(uniq_NORP.values())[i], limit=20, scorer=fuzz.partial_ratio)

#finding more similar words now wherever they have been categorized as ORG
similar_strings_democrats1 = {}
for i in range(len(uniq_NORP.keys())):
    similar_strings_democrats1[i] = process.extract("Democrats", list(uniq_ORG.values())[i], limit=20, scorer=fuzz.partial_ratio)

similar_strings_schiff = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_schiff[i] = process.extract("Schiff", list(uniq_ppl.values())[i], limit = 20, scorer = fuzz.partial_ratio)
    
similar_strings_russia = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_russia[i] = process.extract("Russia", list(uniq_GPE.values())[i], limit = 20, scorer = fuzz.partial_ratio)

#Some funny words used for democrats
similar_strings_republicans = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_republicans[i] = process.extract("Republicans", list(uniq_NORP.values())[i], limit = 20, scorer = fuzz.partial_ratio)

#finding more similar words now wherever they have been categorized as ORG
similar_strings_republicans1 = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_republicans1[i] = process.extract("Republicans", list(uniq_ORG.values())[i], limit = 20, scorer = fuzz.partial_ratio)


similar_strings_hill = {}
for i in range(len(uniq_ppl.keys())):
    similar_strings_hill[i] = process.extract("Hill", list(uniq_ppl.values())[i], limit = 20, scorer = fuzz.partial_ratio)


#~~~~~~~~~~~~~~~~~~~~~ Readability Section ~~~~~~~~~~~~~~~~~~~~~~~~~~~#
    
#Defining a function which will return Flesch and smog score for the text
def givescore(text):
    t = Textatistic(text)
    flesch = t.flesch_score
    print(flesch)
    smog = t.smog_score
    print(smog)

for comment in list(comments_dict.values()):
    givescore(comment[0:10000])

#~~~~~~~~~~~~~~~~~~~~~~~~WORD CLOUD~~~~~~~~~~~~~~~~~~~#
#Feel free to comment out the entire section of word cloud.
path = "/Users/rashmigupta/Desktop/NLP/Final Project/images"
os.chdir(path)
flag_mask = np.array(Image.open("s.png"))
flag_mask

def transform_format(val):
    if val == 0:
        return 255
    else:
        return val
    
# Transform your mask into a new one that will work with the function:
transformed_flag_mask = np.ndarray((flag_mask.shape[0],flag_mask.shape[1]), np.int32)

for i in range(len(flag_mask)):
    transformed_flag_mask[i] = list(map(transform_format, flag_mask[i]))

#transformed_flag_mask
image_colors = ImageColorGenerator(flag_mask)

def GenerateWordCloud(list, i):
    text = ' '.join(list)
    wordcloud = WordCloud(max_font_size=50, max_words=100, 
                          background_color="white", mask = transformed_flag_mask, mode = "RGB").generate(text)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()
    filename = str(i)+"wordcloud.png"
    wordcloud.to_file(filename)
    

# Start with one review:
i = 0
for x in clean_comments:
    GenerateWordCloud(x, i)
    i = i + 1
